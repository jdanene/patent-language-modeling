{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running out of disk memory run <br>\n",
    "`cd /tmp/`<br>\n",
    "`rm -r *`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import tensorflow_datasets as tfds\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from bert import run_classifier_with_tfhub\n",
    "#https://github.com/google-research/bert.git\n",
    "import sys\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import pickle\n",
    "import gc\n",
    "import threading\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All devices:  8\n"
     ]
    }
   ],
   "source": [
    "# Based on -> https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca\n",
    "# Active TPU'1\n",
    "TPU_ADDRESS = \"node-2\"\n",
    "TPU_ZONE = \"us-central1-f\"\n",
    "USE_TPU =True\n",
    "NUM_TPU_CORES = 8\n",
    "#len(tf.config.experimental.list_logical_devices('TPU'))\n",
    "\n",
    "#tf.config.experimental_connect_to_cluster(resolver)\n",
    "#tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "#tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "#print(\"All devices: \", tf.config.experimental.list_logical_devices('TPU'))\n",
    "\n",
    "# Setup TPU related config\n",
    "#tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "#NUM_TPU_CORES = 8\n",
    "\n",
    "#https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=191zq3ZErihP\n",
    "#with tf.Session(TPU_ADDRESS) as session:\n",
    "   # print('TPU devices:')\n",
    "   # pprint.pprint(session.list_devices())    \n",
    "    #contrib.cloud.configure_gcs(session)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDir(bucket, output_dir):\n",
    "    return 'gs://{}/{}'.format(bucket, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: gs://patents-research/bertResults *****\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"bertResults\"\n",
    "DO_DELETE = True\n",
    "USE_BUCKET =True\n",
    "BUCKET = \"patents-research\"\n",
    "\n",
    "if USE_BUCKET:\n",
    "    OUTPUT_DIR = getDir(BUCKET, OUTPUT_DIR)\n",
    "\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "    except:\n",
    "        # doesn't matter if the directory didn't exist\n",
    "        pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Set\n",
    "\n",
    "We classify high impact patents following (Ahuja and Lampert, 2001) as the patents\n",
    " within a given cohort receiving the most citations by other patents within the following 5 year window. Thereafter, for every year we sorted\n",
    "the patents applied for in that year on the basis\n",
    "of their citation weights and identified the top 1\n",
    "percent of patents for that year as breakthrough\n",
    "inventions. This procedure ensures that each patent is compared in its importance only to other\n",
    "patents of the same yea\n",
    "https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPdData(gsPath):\n",
    "    return pd.read_csv(gsPath, sep = \"\\t\")\n",
    "\n",
    "def generateLable(dataset):\n",
    "    # convert to datetime\n",
    "    dataset['publication_date'] = pd.to_datetime(dataset['publication_date'], errors=\"coerce\",format=\"%Y-%m-%d\")\n",
    "    dataset = dataset.sort_values('publication_date', ascending = False)\n",
    "\n",
    "    #drop if date is NaN - only one 1082-03-15\n",
    "    dataset = dataset[dataset.publication_date.isnull() == False]\n",
    "    \n",
    "    # calculate the top 1% by publication date - give it label 1\n",
    "    top1_perc =  dataset.groupby(dataset.publication_date.dt.year)[\"fwrdCitations_5\"].transform(lambda x: x.quantile(.99))\n",
    "    dataset[\"label\"] = dataset[\"fwrdCitations_5\"] >= top1_perc\n",
    "    \n",
    "    # calculate top 5% by publication date - give it label 2\n",
    "    top5_perc = dataset.groupby(dataset.publication_date.dt.year)[\"fwrdCitations_5\"].transform(lambda x: x.quantile(.95))\n",
    "    dataset[\"label\"] = np.where(np.logical_and(dataset[\"fwrdCitations_5\"] >= top5_perc, dataset[\"label\"]==0), 2, dataset[\"label\"])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def saveToGcloud(path,data,isPandas = False ):\n",
    "    '''Saves to gcloud so we dont have to do this long ass step every time'''\n",
    "    if isPandas:\n",
    "        data.to_csv(path, index=False, sep=\"\\t\")\n",
    "    else:\n",
    "        with file_io.FileIO(path, mode='w') as f:\n",
    "            pickle.dump(data,f)\n",
    "\n",
    "\n",
    "def readFromGcloud(path, isPandas = False):\n",
    "    if isPandas:\n",
    "        return pd.read_csv(path,sep=\"\\t\" )\n",
    "    else:\n",
    "        with file_io.FileIO(path, mode='rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "def loadData(loadFromCloud=False):\n",
    "    print(\"Max seq length {}\".format(MAX_SEQ_LENGTH))\n",
    "    if loadFromCloud:\n",
    "        print(f'Loading saved data from cloud!')\n",
    "        train_features = readFromGcloud(TRAIN_TFRecord_PATH)\n",
    "        test_features =readFromGcloud(TEST_TFRecord_PATH)\n",
    "        print(f'Finished loading saved data from cloud!')\n",
    "\n",
    "    else:\n",
    "        print(f'Loading data!')\n",
    "        dataset = loadPdData(DATA_PATH)\n",
    "        print(f'Finised loading data!')\n",
    "        dataset = generateLable(dataset)\n",
    "        print(f'Test/Train Split!')\n",
    "        train,test=train_test_split(dataset, test_size=0.2)\n",
    "        del dataset\n",
    "        gc.collect()\n",
    "        print(f'Finished Test/Train Split!')\n",
    "\n",
    "        print('Saving Test/Train Split to gCloud')\n",
    "        #saveToGloud(TRAIN_DF_PATH,train,isPandas=True)\n",
    "        #saveToGloud(TEST_DF_PATH,test,isPandas=True)\n",
    "        print('Finished Saving Test/Train Split to gCloud!')\n",
    "\n",
    "        print(\"Use the InputExample class from BERT's run_classifier code to create examples from the data\")\n",
    "        train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                           text_a = x[DATA_COLUMN], \n",
    "                                                                           text_b = None, \n",
    "                                                                           label = x[LABEL_COLUMN]), axis = 1)\n",
    "        del train\n",
    "        gc.collect()\n",
    "        test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                           text_a = x[DATA_COLUMN], \n",
    "                                                                           text_b = None, \n",
    "                                                                           label = x[LABEL_COLUMN]), axis = 1)\n",
    "        del test\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Finished using  InputExample class from BERT's run_classifier code to create examples from the data\")\n",
    "\n",
    "        print(\"Convert our train and test features to InputFeatures that BERT understands\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased') # get scientific tokenizer + pointer to the model\n",
    "\n",
    "        train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "        saveToGcloud(TRAIN_TFRecord_PATH,train_features)\n",
    "        del train_InputExamples\n",
    "        gc.collect()\n",
    "        \n",
    "        test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "        saveToGcloud(TEST_TFRecord_PATH,test_features) \n",
    "        del test_InputExamples\n",
    "        gc.collect()\n",
    "        print(\"Finished converting  train and test features to InputFeatures that BERT understands\")\n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the data\n",
    "DATA_PATH = \"gs://patents-research/patent_research/data_frwdcorrect.tsv\"\n",
    "TRAIN_DF_PATH= \"gs://patents-research/patent_research/{}\".format(\"bert_train_df.tsv\") \n",
    "TEST_DF_PATH=\"gs://patents-research/patent_research/{}\".format(\"bert_test_df.tsv\")\n",
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1, 2] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Loading Training \n",
    "**(We load Test asyncronously later)**\n",
    "\n",
    "Converting data into format that bert understands. We use https://github.com/allenai/scibert instead of the standard tokenizer/trained model ref= https://www.aclweb.org/anthology/D19-1371.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUB_MODULE = \"https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_uncased.tar.gz\"\n",
    "TRAIN_TFRecord_PATH= \"gs://patents-research/patent_research/{}\".format(\"train_features_512.pickle\")\n",
    "TEST_TFRecord_PATH= \"gs://patents-research/patent_research/{}\".format(\"test_features_512.pickle\")\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 512 #default sequence is 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved data from cloud!\n",
      "Finished loading saved data from cloud!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_features = readFromGcloud(TRAIN_TFRecord_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download test data \n",
    "\n",
    "Threading this so we can train immediately, and we are not stuck waiting on test data to download when we don't need it immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_downloadTestData(name):\n",
    "    \"\"\"\n",
    "    Worker so we can download test data asynch\n",
    "    \"\"\"\n",
    "    logging.info(\"Thread %s: starting\", name)\n",
    "    global test_features\n",
    "    train_features = readFromGcloud(TEST_TFRecord_PATH)\n",
    "    logging.info(\"Thread %s: finishing\", name)   \n",
    "    \n",
    "# Set up threading \n",
    "format = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(format=format, level=logging.INFO,datefmt=\"%H:%M:%S\")\n",
    "getTestData_thread = threading.Thread(target=downloadTest, args=(1,))\n",
    "getTestData_thread.start() #async download of test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_training, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels, bert_hub_module_handle, dropout):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "    tags = set()\n",
    "    if is_training:\n",
    "        tags.add(\"train\")\n",
    "    bert_module = hub.Module(bert_hub_module_handle, tags=tags, trainable=True)\n",
    "    bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if is_training:\n",
    "            # I.e., 0.1 dropout\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=dropout)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "        return (loss, per_example_loss, logits, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps, use_tpu, bert_hub_module_handle, dropout):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):  \n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        tf.logging.info(\"*** Features ***\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "                is_training, input_ids, input_mask, segment_ids, label_ids, num_labels,\n",
    "                bert_hub_module_handle, dropout)\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = optimization.create_optimizer(\n",
    "                    total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  loss=total_loss,\n",
    "                  train_op=train_op)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "            def metric_fn(per_example_loss, label_ids, logits):\n",
    "                predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predictions)\n",
    "                loss = tf.metrics.mean(per_example_loss)\n",
    "                true_pos = tf.metrics.true_positives(\n",
    "                            label_ids,\n",
    "                            predictions)\n",
    "                true_neg = tf.metrics.true_negatives(\n",
    "                            label_ids,\n",
    "                            predictions)   \n",
    "                false_pos = tf.metrics.false_positives(\n",
    "                            label_ids,\n",
    "                            predictions)  \n",
    "                false_neg = tf.metrics.false_negatives(\n",
    "                            label_ids,\n",
    "                            predictions) \n",
    "                return {\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg,\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"eval_loss\": loss,\n",
    "                }\n",
    "\n",
    "            eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "              mode=mode,\n",
    "              loss=total_loss,\n",
    "              eval_metrics=eval_metrics)\n",
    "        \n",
    "        elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "              mode=mode, predictions={\"probabilities\": probabilities})\n",
    "        else:\n",
    "            raise ValueError(\n",
    "              \"Only TRAIN, EVAL and PREDICT modes are supported: %s\" % (mode))\n",
    "\n",
    "        return output_spec\n",
    "\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL PARAMETERS\n",
    "\n",
    "Example params\n",
    "- https://github.com/google-research/bert/issues/649 \n",
    "- https://github.com/google-research/bert/issues/425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 256\n",
    "EVAL_BATCH_SIZE = NUM_TPU_CORES\n",
    "PREDICT_BATCH_SIZE = NUM_TPU_CORES\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 40.0\n",
    "DROPOUT_KEEP_PROB = .7\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS) \n",
    "#int((800000) / 256 * 40) == about 32k\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up TPU \n",
    "https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=pYVYULZiKvUi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: node-2\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.157.197.34:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 3455271994140814130)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10245224255260713639)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16321600984248760639)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17663426012522639575)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11435110670809846027)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6307267863733612813)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3355277580655669410)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13181272993684034916)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15590471316025377131)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 6836388459525237273)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6802283676312934250)\n"
     ]
    }
   ],
   "source": [
    "# Setupt TPU related config\n",
    "tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS, zone=TPU_ZONE)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n",
    "tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)\n",
    "\n",
    "ITERATIONS_PER_LOOP = 1000\n",
    "\n",
    "# Force TF Hub writes to the GS bucket we provide.\n",
    "os.environ['TFHUB_CACHE_DIR'] =  os.path.join(OUTPUT_DIR,\"tfhub_cache\")\n",
    "tf.gfile.MakeDirs(os.path.join(OUTPUT_DIR,\"tfhub_cache\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Train + Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_config(output_dir):\n",
    "    \"\"\"\n",
    "    Used for run configuration when TPU used\n",
    "    \"\"\"\n",
    "    return tf.contrib.tpu.RunConfig(\n",
    "        cluster=tpu_cluster_resolver,\n",
    "        model_dir=output_dir,\n",
    "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "            iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "            num_shards=NUM_TPU_CORES,\n",
    "            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "def getEstimator(mode_fn):\n",
    "    \"\"\"\n",
    "    Returns the estimator used to train/eval model\n",
    "    \"\"\"\n",
    "    return tf.estimator.tpu.TPUEstimator(\n",
    "          use_tpu=True,\n",
    "          model_fn=mode_fn,\n",
    "          config=get_run_config(OUTPUT_DIR),\n",
    "          train_batch_size=BATCH_SIZE,\n",
    "          eval_batch_size=EVAL_BATCH_SIZE,\n",
    "          predict_batch_size=PREDICT_BATCH_SIZE,\n",
    "          eval_on_tpu = True\n",
    "        ) \n",
    "\n",
    "def model_train(estimator):\n",
    "    \"\"\"\n",
    "    Trains the model, rt only good for TPU\n",
    "    \"\"\"\n",
    "    #Set drop_remainder =True to fix a TPU error\n",
    "    #https://stackoverflow.com/questions/58029896/bert-fine-tuning-with-estimators-on-tpus-on-colab-typeerror-unsupported-operand\n",
    "\n",
    "    print('***** Started training at {} *****'.format(datetime.now()))\n",
    "    print('  Num examples = {}'.format(len(train_features)))\n",
    "    print('  Batch size = {}'.format(BATCH_SIZE))\n",
    "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "    \n",
    "    current_time = datetime.now()\n",
    "    train_input_fn = run_classifier.input_fn_builder(\n",
    "        features=train_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "    print(\"Finished: Training took time \", datetime.now() - current_time)\n",
    "\n",
    "def model_evaluate(estimator):\n",
    "    \"\"\"\n",
    "    Evaluates the model\n",
    "    \"\"\"\n",
    "    print('***** Started evaluation at {} *****'.format(datetime.now()))\n",
    "    print('  Num examples = {}'.format(len(test_features)))\n",
    "    print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
    "\n",
    "    # Eval will be slightly WRONG on the TPU because it will truncate\n",
    "    # the last batch.\n",
    "    eval_steps = int(len(test_features) / EVAL_BATCH_SIZE)\n",
    "    \n",
    "    eval_input_fn = run_classifier.input_fn_builder(\n",
    "        features=test_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=True)\n",
    "    \n",
    "    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "    print('***** Finished evaluation at {} *****'.format(datetime.now()))\n",
    "    output_eval_file = os.path.join(OUTPUT_DIR, \"eval\",\"eval_results.txt\")\n",
    "    tf.gfile.MakeDirs(os.path.join(OUTPUT_DIR, \"eval\"))\n",
    "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "        print(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            print('  {} = {}'.format(key, str(result[key])))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Eval model\n",
    "If evaluate breaks down must rm the lock file in `TFHUB_CACHE_DIR` unless stuck waiting -> https://github.com/tensorflow/hub/issues/579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  dropout = DROPOUT_KEEP_PROB,\n",
    "  use_tpu = USE_TPU,\n",
    "  bert_hub_module_handle = BERT_MODEL_HUB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fd2e456d6a8>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'gs://patents-research/bertResults', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "cluster_def {\n",
      "  job {\n",
      "    name: \"worker\"\n",
      "    tasks {\n",
      "      key: 0\n",
      "      value: \"10.157.197.34:8470\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "isolate_session_state: true\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd17b046c50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.157.197.34:8470', '_evaluation_master': 'grpc://10.157.197.34:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fd1d02322e8>}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "Beginning Training!\n",
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.157.197.34:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 3455271994140814130)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10245224255260713639)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16321600984248760639)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17663426012522639575)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11435110670809846027)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6307267863733612813)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3355277580655669410)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13181272993684034916)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15590471316025377131)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 6836388459525237273)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6802283676312934250)\n",
      "WARNING:tensorflow:From /opt/conda/envs/patent-modeling/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/envs/patent-modeling/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    }
   ],
   "source": [
    "estimator = getEstimator(mode_fn) \n",
    "model_train(estimator)\n",
    "getTestData_thread.join() #wait async download of test data to finish - test data used in `model_evaluate`\n",
    "model_evaluate(estimator)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-modeling",
   "language": "python",
   "name": "patent-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import tensorflow_datasets as tfds\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "#https://github.com/google-research/bert.git\n",
    "import sys\n",
    "sys.path.append('./bert')\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All devices:  8\n"
     ]
    }
   ],
   "source": [
    "# Based on -> https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca\n",
    "# Active TPU's\n",
    "TPU_ADDRESS = \"node-2\"\n",
    "TPU_ZONE = \"us-central1-f\"\n",
    "USE_TPU =True\n",
    "NUM_TPU_CORES = len(tf.config.experimental.list_logical_devices('TPU'))\n",
    "\n",
    "#tf.config.experimental_connect_to_cluster(resolver)\n",
    "#tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "#tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "#print(\"All devices: \", tf.config.experimental.list_logical_devices('TPU'))\n",
    "\n",
    "# Setup TPU related config\n",
    "#tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "#NUM_TPU_CORES = 8\n",
    "\n",
    "#https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=191zq3ZErihP\n",
    "#with tf.Session(TPU_ADDRESS) as session:\n",
    "   # print('TPU devices:')\n",
    "   # pprint.pprint(session.list_devices())    \n",
    "    #contrib.cloud.configure_gcs(session)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDir(bucket, output_dir):\n",
    "    return 'gs://{}/{}'.format(bucket, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: gs://patents-research/bertResults *****\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"bertResults\"\n",
    "DO_DELETE = False\n",
    "USE_BUCKET =True\n",
    "BUCKET = \"patents-research\"\n",
    "\n",
    "if USE_BUCKET:\n",
    "    OUTPUT_DIR = getDir(BUCKET, OUTPUT_DIR)\n",
    "\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "    except:\n",
    "        # doesn't matter if the directory didn't exist\n",
    "        pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Set\n",
    "\n",
    "We classify high impact patents following (Ahuja and Lampert, 2001) as the patents\n",
    " within a given cohort receiving the most citations by other patents within the following 5 year window. Thereafter, for every year we sorted\n",
    "the patents applied for in that year on the basis\n",
    "of their citation weights and identified the top 1\n",
    "percent of patents for that year as breakthrough\n",
    "inventions. This procedure ensures that each patent is compared in its importance only to other\n",
    "patents of the same yea\n",
    "https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(gsPath):\n",
    "    return pd.read_csv(gsPath, sep = \"\\t\")\n",
    "\n",
    "def generateLable(dataset):\n",
    "    # convert to datetime\n",
    "    dataset['publication_date'] = pd.to_datetime(dataset['publication_date'], errors=\"coerce\",format=\"%Y-%m-%d\")\n",
    "    dataset = dataset.sort_values('publication_date', ascending = False)\n",
    "\n",
    "    #drop if date is NaN - only one 1082-03-15\n",
    "    dataset = dataset[dataset.publication_date.isnull() == False]\n",
    "    \n",
    "    # calculate the top 1% by publication date - give it label 1\n",
    "    top1_perc =  dataset.groupby(dataset.publication_date.dt.year)[\"fwrdCitations_5\"].transform(lambda x: x.quantile(.99))\n",
    "    dataset[\"label\"] = dataset[\"fwrdCitations_5\"] >= top1_perc\n",
    "    \n",
    "    # calculate top 5% by publication date - give it label 2\n",
    "    top5_perc = dataset.groupby(dataset.publication_date.dt.year)[\"fwrdCitations_5\"].transform(lambda x: x.quantile(.95))\n",
    "    dataset[\"label\"] = np.where(np.logical_and(dataset[\"fwrdCitations_5\"] >= top5_perc, dataset[\"label\"]==0), 2, dataset[\"label\"])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def saveToGcloud(path,data,isPandas = False ):\n",
    "    '''Saves to gcloud so we dont have to do this long ass step every time'''\n",
    "    if isPandas:\n",
    "        data.to_csv(path, index=False, sep=\"\\t\")\n",
    "    else:\n",
    "        with file_io.FileIO(path, mode='w') as f:\n",
    "            pickle.dump(data,f)\n",
    "\n",
    "\n",
    "def readFromGcloud(path, isPandas = False):\n",
    "    if isPandas:\n",
    "        return pd.read_csv(path,sep=\"\\t\" )\n",
    "    else:\n",
    "        with file_io.FileIO(path, mode='rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "def loadData(loadFromCloud=False):\n",
    "    \n",
    "    if loadFromCloud:\n",
    "        print(f'Loading saved data from cloud!')\n",
    "        train_features = readTFRecord(TRAIN_TFRecord_PATH)\n",
    "        test_features =readTFRecord(TEST_TFRecord_PATH)\n",
    "        print(f'Finished loading saved data from cloud!')\n",
    "\n",
    "    else:\n",
    "        print(f'Loading data!')\n",
    "        dataset = loadData(DATA_PATH)\n",
    "        print(f'Finised loading data!')\n",
    "        dataset = generateLable(dataset)\n",
    "        print(f'Test/Train Split!')\n",
    "        train,test=train_test_split(dataset, test_size=0.2)\n",
    "        print(f'Finished Test/Train Split!')\n",
    "\n",
    "        print('Saving Test/Train Split to gCloud')\n",
    "        saveToGloud(TRAIN_DF_PATH,train,isPandas=True)\n",
    "        saveToGloud(TEST_DF_PATH,test,isPandas=True)\n",
    "        print('Finished Saving Test/Train Split to gCloud!')\n",
    "\n",
    "        print(\"Use the InputExample class from BERT's run_classifier code to create examples from the data\")\n",
    "        train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                           text_a = x[DATA_COLUMN], \n",
    "                                                                           text_b = None, \n",
    "                                                                           label = x[LABEL_COLUMN]), axis = 1)\n",
    "        test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                           text_a = x[DATA_COLUMN], \n",
    "                                                                           text_b = None, \n",
    "                                                                           label = x[LABEL_COLUMN]), axis = 1)\n",
    "        print(\"Finished using  InputExample class from BERT's run_classifier code to create examples from the data\")\n",
    "\n",
    "        print(\"Convert our train and test features to InputFeatures that BERT understands\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased') # get scientific tokenizer + pointer to the model\n",
    "\n",
    "        train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "        saveToGloud(TRAIN_TFRecord_PATH,train_features)\n",
    "\n",
    "        test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "        saveToGloud(TEST_TFRecord_PATH,test_features) \n",
    "        print(\"Finished converting  train and test features to InputFeatures that BERT understands\")\n",
    "    return train_features, test_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the data\n",
    "DATA_PATH = \"gs://patents-research/patent_research/data_frwdcorrect.tsv\"\n",
    "TRAIN_DF_PATH= \"gs://patents-research/patent_research/{}\".format(\"bert_train_df.tsv\") \n",
    "TEST_DF_PATH=\"gs://patents-research/patent_research/{}\".format(\"bert_test_df.tsv\")\n",
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1, 2] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Converting data into format that bert understands. We use https://github.com/allenai/scibert instead of the standard tokenizer/trained model ref= https://www.aclweb.org/anthology/D19-1371.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUB_MODULE = \"https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_uncased.tar.gz\"\n",
    "TRAIN_TFRecord_PATH= \"gs://patents-research/patent_research/{}\".format(\"train_features.pickle\")\n",
    "TEST_TFRecord_PATH= \"gs://patents-research/patent_research/{}\".format(\"test_features.pickle\")\n",
    "HUB_MODULE = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved data from cloud!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_features, test_features = loadData(loadFromCloud=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels, dropout):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(\n",
    "      \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "      trainable=True)\n",
    "    bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "    initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=dropout)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/config/experimental/list_logical_devices\n",
    " \n",
    "    #TPUEstimatorSpec\n",
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,num_warmup_steps, dropout = 0.9, use_tpu=False):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "        spec = {True:tf.estimator.tpu.TPUEstimatorSpec, False:tf.estimator.EstimatorSpec }\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "              is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels, dropout)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "                  loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=use_tpu)\n",
    "          # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                auc = tf.metrics.auc(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                recall = tf.metrics.recall(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                precision = tf.metrics.precision(\n",
    "                    label_ids,\n",
    "                    predicted_labels) \n",
    "                true_pos = tf.metrics.true_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)   \n",
    "                false_pos = tf.metrics.false_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)  \n",
    "                false_neg = tf.metrics.false_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }  \n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return spec[use_tpu](mode=mode,loss=loss,train_op=train_op)\n",
    "            else:\n",
    "                return spec[use_tpu](mode=mode,loss=loss,eval_metric_ops=eval_metrics)                \n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "            predictions = {\n",
    "                    'probabilities': log_probs,\n",
    "                    'labels': predicted_labels\n",
    "              }\n",
    "            return spec[use_tpu](mode, predictions=predictions)\n",
    "\n",
    "      # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = NUM_TPU_CORES\n",
    "PREDICT_BATCH_SIZE = NUM_TPU_CORES\n",
    "if USE_TPU:#When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. \n",
    "    BATCH_SIZE*=NUM_TPU_CORES\n",
    "    \n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "DROPOUT_KEEP_PROB = .7\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tpu related configs: Based on https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=pYVYULZiKvUi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setupt TPU related config\n",
    "tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS, zone=TPU_ZONE)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)\n",
    "\n",
    "ITERATIONS_PER_LOOP = 1000\n",
    "# Force TF Hub writes to the GS bucket we provide.\n",
    "os.environ['TFHUB_CACHE_DIR'] =  OUTPUT_DIR\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FN = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  dropout = DROPOUT_KEEP_PROB,\n",
    "  use_tpu = USE_TPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Train + Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_config(output_dir):\n",
    "    \"\"\"\n",
    "    Used for run configuration when TPU used\n",
    "    \"\"\"\n",
    "    return tf.contrib.tpu.RunConfig(\n",
    "        cluster=tpu_cluster_resolver,\n",
    "        model_dir=output_dir,\n",
    "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "            iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "            num_shards=NUM_TPU_CORES,\n",
    "            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "def getEstimator():\n",
    "    \"\"\"\n",
    "    Returns the estimator used to train/eval model\n",
    "    \"\"\"\n",
    "    if USE_TPU:\n",
    "        estimator = tf.estimator.tpu.TPUEstimator(\n",
    "          use_tpu=USE_TPU,\n",
    "          model_fn=MODEL_FN,\n",
    "          config=get_run_config(OUTPUT_DIR),\n",
    "          train_batch_size=BATCH_SIZE,\n",
    "          eval_batch_size=EVAL_BATCH_SIZE,\n",
    "          predict_batch_size=PREDICT_BATCH_SIZE,\n",
    "        )\n",
    "    else: \n",
    "        run_config = tf.estimator.RunConfig(\n",
    "            model_dir=OUTPUT_DIR,\n",
    "            save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "            save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "    \n",
    "        estimator = tf.estimator.Estimator(\n",
    "          model_fn=MODEL_FN,\n",
    "          config=run_config,\n",
    "          params={\"batch_size\": BATCH_SIZE})\n",
    "    return estimator\n",
    "\n",
    "def model_train(estimator):\n",
    "    \"\"\"\n",
    "    Trains the model, rt only good for TPU\n",
    "    \"\"\"\n",
    "    #Set drop_remainder =True to fix a TPU error\n",
    "    #https://stackoverflow.com/questions/58029896/bert-fine-tuning-with-estimators-on-tpus-on-colab-typeerror-unsupported-operand\n",
    "\n",
    "    print(f'Beginning Training!')\n",
    "    current_time = datetime.datetime.now()\n",
    "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "        features=train_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=True,\n",
    "        drop_remainder=USE_TPU)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "    print(\"Training took time \", datetime.now() - current_time)\n",
    "\n",
    "def model_evaluate(estimator):\n",
    "    \"\"\"\n",
    "    Evaluates the model\n",
    "    \"\"\"\n",
    "    print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "\n",
    "    eval_steps = int(len(test_features) / EVAL_BATCH_SIZE)\n",
    "    \n",
    "    eval_input_fn = run_classifier.input_fn_builder(\n",
    "        features=test_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=True)\n",
    "    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "    print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "\n",
    "    output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
    "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "        print(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            print('  {} = {}'.format(key, str(result[key])))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = getEstimator()\n",
    "model_train(estimator)\n",
    "model_evaluate(estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-modeling",
   "language": "python",
   "name": "patent-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

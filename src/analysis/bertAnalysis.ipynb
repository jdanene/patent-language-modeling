{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import tensorflow_datasets as tfds\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "#https://github.com/google-research/bert.git\n",
    "import sys\n",
    "sys.path.append('./bert')\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: node-2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bc01afa065fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTPU_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTPU_ZONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_connect_to_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_tpu_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtpu_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All devices: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_logical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/patent-modeling/lib/python3.7/site-packages/tensorflow_core/python/tpu/tpu_strategy_util.py\u001b[0m in \u001b[0;36minitialize_tpu_system\u001b[0;34m(cluster_resolver)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msession_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mserialized_topology\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished initializing TPU system.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/patent-modeling/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/patent-modeling/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/patent-modeling/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/patent-modeling/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/patent-modeling/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/patent-modeling/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Based on -> https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca\n",
    "# Active TPU's\n",
    "TPU_ADDRESS = \"node-2\"\n",
    "TPU_ZONE = \"us-central1-f\"\n",
    "USE_TPU =True\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "#tf.config.experimental_connect_to_cluster(resolver)\n",
    "#tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "#tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "#print(\"All devices: \", tf.config.experimental.list_logical_devices('TPU'))\n",
    "\n",
    "# Setup TPU related config\n",
    "#tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "#NUM_TPU_CORES = 8\n",
    "\n",
    "#https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=191zq3ZErihP\n",
    "#with tf.Session(TPU_ADDRESS) as session:\n",
    "   # print('TPU devices:')\n",
    "   # pprint.pprint(session.list_devices())    \n",
    "    #contrib.cloud.configure_gcs(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDir(bucket, output_dir):\n",
    "    return 'gs://{}/{}'.format(bucket, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"bertResults\"\n",
    "DO_DELETE = False\n",
    "USE_BUCKET =True\n",
    "BUCKET = \"patents-research\"\n",
    "\n",
    "if USE_BUCKET:\n",
    "    OUTPUT_DIR = getDir(BUCKET, OUTPUT_DIR)\n",
    "\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "    except:\n",
    "        # doesn't matter if the directory didn't exist\n",
    "        pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Set\n",
    "\n",
    "We classify high impact patents following (Ahuja and Lampert, 2001) as the patents\n",
    " within a given cohort receiving the most citations by other patents within the following 5 year window. Thereafter, for every year we sorted\n",
    "the patents applied for in that year on the basis\n",
    "of their citation weights and identified the top 1\n",
    "percent of patents for that year as breakthrough\n",
    "inventions. This procedure ensures that each patent is compared in its importance only to other\n",
    "patents of the same yea\n",
    "https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(gsPath):\n",
    "    return pd.read_csv(gsPath, sep = \"\\t\")\n",
    "\n",
    "def generateLable(dataset):\n",
    "    # convert to datetime\n",
    "    dataset['publication_date'] = pd.to_datetime(dataset['publication_date'], errors=\"coerce\",format=\"%Y-%m-%d\")\n",
    "    dataset = dataset.sort_values('publication_date', ascending = False)\n",
    "\n",
    "    #drop if date is NaN - only one 1082-03-15\n",
    "    dataset = dataset[dataset.publication_date.isnull() == False]\n",
    "    \n",
    "    # calculate the top 1% by publication date - give it label 1\n",
    "    top1_perc =  dataset.groupby(dataset.publication_date.dt.year)[\"fwrdCitations_5\"].transform(lambda x: x.quantile(.99))\n",
    "    dataset[\"label\"] = dataset[\"fwrdCitations_5\"] >= top1_perc\n",
    "    \n",
    "    # calculate top 5% by publication date - give it label 2\n",
    "    top5_perc = dataset.groupby(dataset.publication_date.dt.year)[\"fwrdCitations_5\"].transform(lambda x: x.quantile(.95))\n",
    "    dataset[\"label\"] = np.where(np.logical_and(dataset[\"fwrdCitations_5\"] >= top5_perc, dataset[\"label\"]==0), 2, dataset[\"label\"])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def saveToGloud(path,data):\n",
    "    '''Saves to gcloud so we dont have to do this long ass step every time'''\n",
    "    with file_io.FileIO(path, mode='w') as f:\n",
    "        pickle.dump(data,f)\n",
    "\n",
    "\n",
    "def readTFRecord(path):\n",
    "    with file_io.FileIO(path, mode='rb') as f:\n",
    "        return pickle.load(f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: gs://patents-research/bertResults *****\n"
     ]
    }
   ],
   "source": [
    "# The path to the data\n",
    "DATA_PATH = \"gs://patents-research/patent_research/data_frwdcorrect.tsv\"\n",
    "TRAIN_DF_PATH= \"gs://patents-research/patent_research/{}\".format(\"bert_train_df.tsv\")\n",
    "TEST_DF_PATH=\"gs://patents-research/patent_research/{}\".format(\"bert_test_df.tsv\")\n",
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1, 2] \n",
    "\n",
    "\n",
    "print(f'Loading data!')\n",
    "dataset = loadData(DATA_PATH)\n",
    "print(f'Finised loading data!')\n",
    "dataset = generateLable(dataset)\n",
    "print(f'Test/Train Split!')\n",
    "train,test=train_test_split(dataset, test_size=0.2)\n",
    "print(f'Finished Test/Train Split!')\n",
    "\n",
    "print('Saving Test/Train Split to gCloud')\n",
    "saveToGloud(TRAIN_DF_PATH,train,isPandas=True)\n",
    "saveToGloud(TEST_DF_PATH,test,isPandas=True)\n",
    "print('Finished Saving Test/Train Split to gCloud!')\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Converting data into format that bert understands. We use https://github.com/allenai/scibert instead of the standard tokenizer/trained model ref= https://www.aclweb.org/anthology/D19-1371.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HUB_MODULE = \"https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_uncased.tar.gz\"\n",
    "TRAIN_TFRecord_PATH= \"gs://patents-research/patent_research/{}\".format(\"train_features.pickle\")\n",
    "TEST_TFRecord_PATH= \"gs://patents-research/patent_research/{}\".format(\"test_features.pickle\")\n",
    "HUB_MODULE = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Use the InputExample class from BERT's run_classifier code to create examples from the data\")\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "print(\"Finished using  InputExample class from BERT's run_classifier code to create examples from the data\")\n",
    "\n",
    "\n",
    "print(\"Convert our train and test features to InputFeatures that BERT understands\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased') # get scientific tokenizer + pointer to the model\n",
    "\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "saveToGloud(TRAIN_TFRecord_PATH,train_features)\n",
    "\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "saveToGloud(TEST_TFRecord_PATH,test_features) \n",
    "\n",
    "train_features = readTFRecord(TRAIN_TFRecord_PATH)\n",
    "test_features =readTFRecord(TEST_TFRecord_PATH)\n",
    "\n",
    "print(\"Finsed converting  train and test features to InputFeatures that BERT understands\")\n",
    "\n",
    "# garbage collection\n",
    "del train_InputExamples\n",
    "del test_InputExamples\n",
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels, dropout):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(\n",
    "      \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "      trainable=True)\n",
    "    bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "    initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=dropout)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/config/experimental/list_logical_devices\n",
    " \n",
    "    #TPUEstimatorSpec\n",
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,num_warmup_steps, dropout = 0.9, use_tpu=False):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "        spec = {True:tf.estimator.tpu.TPUEstimatorSpec, False:tf.estimator.EstimatorSpec }\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "              is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels, dropout)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "                  loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=use_tpu)\n",
    "          # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                auc = tf.metrics.auc(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                recall = tf.metrics.recall(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                precision = tf.metrics.precision(\n",
    "                    label_ids,\n",
    "                    predicted_labels) \n",
    "                true_pos = tf.metrics.true_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)   \n",
    "                false_pos = tf.metrics.false_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)  \n",
    "                false_neg = tf.metrics.false_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }  \n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return spec[use_tpu](mode=mode,loss=loss,train_op=train_op)\n",
    "            else:\n",
    "                return spec[use_tpu](mode=mode,loss=loss,eval_metric_ops=eval_metrics)                \n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "            predictions = {\n",
    "                    'probabilities': log_probs,\n",
    "                    'labels': predicted_labels\n",
    "              }\n",
    "            return spec[use_tpu](mode, predictions=predictions)\n",
    "\n",
    "      # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = NUM_TPU_CORES\n",
    "PREDICT_BATCH_SIZE = NUM_TPU_CORES\n",
    "if USE_TPU:#When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. \n",
    "    BATCH_SIZE*=NUM_TPU_CORES\n",
    "    \n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "DROPOUT_KEEP_PROB = .7\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tpu related configs: Based on https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=pYVYULZiKvUi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setupt TPU related config\n",
    "tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS, zone=TPU_ZONE)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)\n",
    "\n",
    "ITERATIONS_PER_LOOP = 1000\n",
    "# Force TF Hub writes to the GS bucket we provide.\n",
    "os.environ['TFHUB_CACHE_DIR'] =  OUTPUT_DIR\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FN = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  dropout = DROPOUT_KEEP_PROB,\n",
    "  use_tpu = USE_TPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Train + Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_config(output_dir):\n",
    "    \"\"\"\n",
    "    Used for run configuration when TPU used\n",
    "    \"\"\"\n",
    "    return tf.contrib.tpu.RunConfig(\n",
    "        cluster=tpu_cluster_resolver,\n",
    "        model_dir=output_dir,\n",
    "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "            iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "            num_shards=NUM_TPU_CORES,\n",
    "            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "def getEstimator():\n",
    "    \"\"\"\n",
    "    Returns the estimator used to train/eval model\n",
    "    \"\"\"\n",
    "    if USE_TPU:\n",
    "        estimator = tf.estimator.tpu.TPUEstimator(\n",
    "          use_tpu=USE_TPU,\n",
    "          model_fn=MODEL_FN,\n",
    "          config=get_run_config(OUTPUT_DIR),\n",
    "          train_batch_size=BATCH_SIZE,\n",
    "          eval_batch_size=EVAL_BATCH_SIZE,\n",
    "          predict_batch_size=PREDICT_BATCH_SIZE,\n",
    "        )\n",
    "    else: \n",
    "        run_config = tf.estimator.RunConfig(\n",
    "            model_dir=OUTPUT_DIR,\n",
    "            save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "            save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "    \n",
    "        estimator = tf.estimator.Estimator(\n",
    "          model_fn=MODEL_FN,\n",
    "          config=run_config,\n",
    "          params={\"batch_size\": BATCH_SIZE})\n",
    "    return estimator\n",
    "\n",
    "def model_train(estimator):\n",
    "    \"\"\"\n",
    "    Trains the model, rt only good for TPU\n",
    "    \"\"\"\n",
    "    #Set drop_remainder =True to fix a TPU error\n",
    "    #https://stackoverflow.com/questions/58029896/bert-fine-tuning-with-estimators-on-tpus-on-colab-typeerror-unsupported-operand\n",
    "\n",
    "    print(f'Beginning Training!')\n",
    "    current_time = datetime.datetime.now()\n",
    "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "        features=train_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=True,\n",
    "        drop_remainder=USE_TPU)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "    print(\"Training took time \", datetime.now() - current_time)\n",
    "\n",
    "def model_evaluate(estimator):\n",
    "    \"\"\"\n",
    "    Evaluates the model\n",
    "    \"\"\"\n",
    "    print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "\n",
    "    eval_steps = int(len(test_features) / EVAL_BATCH_SIZE)\n",
    "    \n",
    "    eval_input_fn = run_classifier.input_fn_builder(\n",
    "        features=test_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=True)\n",
    "    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "    print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "\n",
    "    output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
    "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "        print(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            print('  {} = {}'.format(key, str(result[key])))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = getEstimator()\n",
    "model_train(estimator)\n",
    "model_evaluate(estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-modeling",
   "language": "python",
   "name": "patent-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
